{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ AI Visual Search - Custom Dataset Generator\n",
                "\n",
                "Since public datasets are broken/private, we will **create our own**!\n",
                "\n",
                "This notebook downloads images directly from the web for the parts YOU want.\n",
                "\n",
                "**Parts Included**: Battery, Brake Pads, Engine, Alternator, etc.\n",
                "\n",
                "---\n",
                "\n",
                "## Setup Instructions\n",
                "\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\n",
                "2. **Run all cells** in order\n",
                "3. **Wait for download** (takes ~5-10 mins)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tensorflow pillow matplotlib scikit-learn bing-image-downloader\n",
                "\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "import os\n",
                "from bing_image_downloader import downloader\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Define Your Car Parts\n",
                "\n",
                "Add any car part you want to this list!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List of parts to download\n",
                "CAR_PARTS = [\n",
                "    \"car battery\",\n",
                "    \"car brake pads\",\n",
                "    \"car engine\",\n",
                "    \"car alternator\",\n",
                "    \"car radiator\",\n",
                "    \"car spark plug\",\n",
                "    \"car oil filter\",\n",
                "    \"car air filter\",\n",
                "    \"car headlight\",\n",
                "    \"car tire\",\n",
                "    \"car steering wheel\",\n",
                "    \"car turbocharger\",\n",
                "    \"car suspension shock absorber\",\n",
                "    \"car exhaust pipe\",\n",
                "    \"car transmission gearbox\"\n",
                "]\n",
                "\n",
                "print(f\"We will download images for {len(CAR_PARTS)} categories.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Download Images (Automated)\n",
                "\n",
                "This downloads 50 images per category."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create dataset directory\n",
                "!mkdir -p dataset/train\n",
                "\n",
                "for part in CAR_PARTS:\n",
                "    print(f\"\\n‚¨áÔ∏è Downloading {part}...\")\n",
                "    \n",
                "    # Download 60 images (we'll use 50 for train, 10 for test)\n",
                "    downloader.download(\n",
                "        part, \n",
                "        limit=60, \n",
                "        output_dir='dataset_raw', \n",
                "        adult_filter_off=True, \n",
                "        force_replace=False, \n",
                "        timeout=60,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "print(\"\\n‚úì All images downloaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Organize & Clean Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from PIL import Image\n",
                "\n",
                "# Create train/test structure\n",
                "!mkdir -p dataset/train dataset/test\n",
                "\n",
                "def is_valid_image(path):\n",
                "    try:\n",
                "        img = Image.open(path)\n",
                "        img.verify()\n",
                "        return True\n",
                "    except:\n",
                "        return False\n",
                "\n",
                "for part in CAR_PARTS:\n",
                "    # Clean category name (remove 'car ' prefix)\n",
                "    clean_name = part.replace('car ', '').replace(' ', '_')\n",
                "    \n",
                "    # Create class folders\n",
                "    os.makedirs(f'dataset/train/{clean_name}', exist_ok=True)\n",
                "    os.makedirs(f'dataset/test/{clean_name}', exist_ok=True)\n",
                "    \n",
                "    # Get downloaded images\n",
                "    src_dir = f'dataset_raw/{part}'\n",
                "    if not os.path.exists(src_dir): continue\n",
                "        \n",
                "    images = [f for f in os.listdir(src_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
                "    \n",
                "    valid_images = []\n",
                "    for img in images:\n",
                "        src_path = os.path.join(src_dir, img)\n",
                "        if is_valid_image(src_path):\n",
                "            valid_images.append(img)\n",
                "            \n",
                "    # Split 80/20\n",
                "    split_idx = int(len(valid_images) * 0.8)\n",
                "    train_imgs = valid_images[:split_idx]\n",
                "    test_imgs = valid_images[split_idx:]\n",
                "    \n",
                "    # Move files\n",
                "    for img in train_imgs:\n",
                "        shutil.copy(os.path.join(src_dir, img), f'dataset/train/{clean_name}/{img}')\n",
                "        \n",
                "    for img in test_imgs:\n",
                "        shutil.copy(os.path.join(src_dir, img), f'dataset/test/{clean_name}/{img}')\n",
                "        \n",
                "    print(f\"{clean_name}: {len(train_imgs)} train, {len(test_imgs)} test\")\n",
                "\n",
                "print(\"\\n‚úì Dataset organized!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Prepare Data Generators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "IMG_SIZE = 224\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 20\n",
                "\n",
                "# Data augmentation\n",
                "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    rotation_range=20,\n",
                "    width_shift_range=0.2,\n",
                "    height_shift_range=0.2,\n",
                "    horizontal_flip=True,\n",
                "    zoom_range=0.2,\n",
                "    brightness_range=[0.8, 1.2]\n",
                ")\n",
                "\n",
                "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    'dataset/train',\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical'\n",
                ")\n",
                "\n",
                "val_generator = val_datagen.flow_from_directory(\n",
                "    'dataset/test',\n",
                "    target_size=(IMG_SIZE, IMG_SIZE),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical'\n",
                ")\n",
                "\n",
                "num_classes = len(train_generator.class_indices)\n",
                "print(f\"\\nClasses ({num_classes}): {list(train_generator.class_indices.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Build Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_model = tf.keras.applications.EfficientNetB0(\n",
                "    include_top=False,\n",
                "    weights='imagenet',\n",
                "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
                ")\n",
                "\n",
                "base_model.trainable = False\n",
                "\n",
                "model = tf.keras.Sequential([\n",
                "    base_model,\n",
                "    tf.keras.layers.GlobalAveragePooling2D(),\n",
                "    tf.keras.layers.Dropout(0.3),\n",
                "    tf.keras.layers.Dense(512, activation='relu'),\n",
                "    tf.keras.layers.Dropout(0.3),\n",
                "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
                "])\n",
                "\n",
                "model.compile(\n",
                "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "callbacks = [\n",
                "    tf.keras.callbacks.ModelCheckpoint(\n",
                "        'best_model.h5',\n",
                "        monitor='val_accuracy',\n",
                "        save_best_only=True,\n",
                "        verbose=1\n",
                "    ),\n",
                "    tf.keras.callbacks.EarlyStopping(\n",
                "        monitor='val_loss',\n",
                "        patience=5,\n",
                "        restore_best_weights=True\n",
                "    )\n",
                "]\n",
                "\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=val_generator,\n",
                "    callbacks=callbacks\n",
                ")\n",
                "\n",
                "print(\"\\n‚úì Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Save & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "model.save('visual_search_model.h5')\n",
                "\n",
                "# Save labels\n",
                "class_labels = {v: k for k, v in train_generator.class_indices.items()}\n",
                "import json\n",
                "with open('class_labels.json', 'w') as f:\n",
                "    json.dump(class_labels, f, indent=2)\n",
                "\n",
                "# Save info\n",
                "model_info = {\n",
                "    'model': 'EfficientNet-B0',\n",
                "    'num_classes': num_classes,\n",
                "    'categories': list(train_generator.class_indices.keys())\n",
                "}\n",
                "with open('model_info.json', 'w') as f:\n",
                "    json.dump(model_info, f, indent=2)\n",
                "\n",
                "from google.colab import files\n",
                "files.download('visual_search_model.h5')\n",
                "files.download('class_labels.json')\n",
                "files.download('model_info.json')\n",
                "\n",
                "print(\"\\n‚úì All files downloaded!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}